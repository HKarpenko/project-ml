{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'popularity'\n",
    "\n",
    "features =  ['valence',\n",
    " 'acousticness',\n",
    " 'artists',\n",
    " 'danceability',\n",
    " 'duration_ms',\n",
    " 'energy',\n",
    " 'explicit',\n",
    " 'instrumentalness',\n",
    " 'liveness',\n",
    " 'loudness',\n",
    " 'mode',\n",
    " 'speechiness',\n",
    " 'tempo']\n",
    "\n",
    "data_old_path = \"data/old_VS_new/old_era_data.csv\"\n",
    "data_new_path = \"data/old_VS_new/new_era_data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answers(answer, actual):\n",
    "    answer = list(answer)\n",
    "    actual = list(actual)\n",
    "    ok = 0\n",
    "    diff = 15\n",
    "    ans_list = []\n",
    "    for i in range(len(answer)):\n",
    "        if(actual[i] < answer[i]+diff and actual[i] > answer[i]-diff ):\n",
    "            ok+=1\n",
    "            ans_list.append(1)\n",
    "        else:\n",
    "            ans_list.append(0)\n",
    "    return ok/len(answer)\n",
    "\n",
    "def get_accuracy(classifier, test):\n",
    "    X = test[\"x\"]\n",
    "    Y = test[\"y\"]\n",
    "    ans = classifier.predict(X)\n",
    "\n",
    "    return check_answers(ans, Y)\n",
    "    \n",
    "\n",
    "def print_features(classifier):\n",
    "    feature_importances_df = pd.DataFrame(classifier.feature_importances_, columns=['importance'], \n",
    "        index = features).sort_values('importance', ascending=False)\n",
    "\n",
    "    print(feature_importances_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACC:  0.8596894238720941  Depth:  13  Criterion:  entropy\n                  importance\nartists             0.193919\nacousticness        0.132311\nspeechiness         0.115604\nduration_ms         0.086969\nloudness            0.084677\ndanceability        0.074824\ninstrumentalness    0.069613\nvalence             0.066156\nenergy              0.062243\nliveness            0.056045\ntempo               0.051040\nmode                0.006520\nexplicit            0.000078\n"
     ]
    }
   ],
   "source": [
    "def best_tree_crit(features, df):\n",
    "    best_options = {\"val\" : 0, \"crit\" : \"gini\", \"depth\" : 10}\n",
    "    for cr in [\"gini\", 'entropy']:\n",
    "        for dp in range(1,15):\n",
    "            myTree = tree.DecisionTreeClassifier(criterion=cr, max_depth=dp)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "            myTree.fit(X_train, y_train)\n",
    "\n",
    "            acc = get_accuracy(myTree, { \"x\" : X_test, \"y\" : y_test })\n",
    "\n",
    "            if acc > best_options[\"val\"] :\n",
    "                best_options[\"val\"] = acc\n",
    "                best_options[\"crit\"] = cr\n",
    "                best_options[\"depth\"] = dp\n",
    "    return best_options\n",
    "            \n",
    "\n",
    "def create_tree(features, path):\n",
    "    df = pd.read_csv(str(path))\n",
    "    \n",
    "    #best_options = best_tree_crit(features, df)\n",
    "\n",
    "    best_options = {\"val\" : None, \"crit\" : \"entropy\", \"depth\" : 13} #best for old data\n",
    "\n",
    "    myTree = tree.DecisionTreeClassifier(criterion=best_options['crit'], max_depth=best_options['depth'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "    myTree.fit(X_train, y_train)\n",
    "    acc = get_accuracy(myTree, { \"x\" : X_test, \"y\" : y_test })\n",
    "\n",
    "    print(\"ACC: \",acc, \" Depth: \",best_options['depth'], \" Criterion: \",best_options['crit'])\n",
    "    print_features(myTree)\n",
    "\n",
    "create_tree(features, data_old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "ACC:  0.8283561517963797  Estimators:  1  Algorithm:  SAMME.R\n                  importance\nacousticness             1.0\nvalence                  0.0\nartists                  0.0\ndanceability             0.0\nduration_ms              0.0\nenergy                   0.0\nexplicit                 0.0\ninstrumentalness         0.0\nliveness                 0.0\nloudness                 0.0\nmode                     0.0\nspeechiness              0.0\ntempo                    0.0\n"
     ]
    }
   ],
   "source": [
    "def best_ada_crit(features, df):\n",
    "    best_options = {\"val\" : 0, \"ests\" : 10, \"alg\":\"SAMME.R\"}\n",
    "    for al in [\"SAMME.R\", \"SAMME\"] :\n",
    "        for es in range(1,201,2):\n",
    "            adb = AdaBoostClassifier(n_estimators=es, algorithm=al)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], \n",
    "                                                    test_size=0.2, random_state=1)\n",
    "\n",
    "            adb.fit(X_train, y_train)\n",
    "\n",
    "            acc = get_accuracy(adb, { \"x\" : X_test, \"y\" : y_test })\n",
    "\n",
    "            if acc > best_options[\"val\"] :\n",
    "                best_options[\"val\"] = acc\n",
    "                best_options[\"ests\"] = es\n",
    "                best_options[\"alg\"] = al\n",
    "    return best_options\n",
    "\n",
    "def create_ada(features, path):\n",
    "    df = pd.read_csv(str(path))\n",
    "    best_options = best_ada_crit(features, df)\n",
    "\n",
    "    adb = AdaBoostClassifier(n_estimators=best_options['ests'], algorithm=best_options['alg'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], \n",
    "                                            test_size=0.2, random_state=1)\n",
    "    adb.fit(X_train, y_train)\n",
    "    acc = get_accuracy(adb, { \"x\" : X_test, \"y\" : y_test })\n",
    "    \n",
    "    print(\"ACC: \",acc, \" Estimators: \",best_options['ests'], \" Algorithm: \",best_options['alg'])\n",
    "    print_features(adb)\n",
    "\n",
    "create_ada(features, data_old_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def best_rdf_crit(features, df):\n",
    "    best_options = {\"val\" : 0, \"ests\" : 10, \"crit\":\"gini\", \"depth\":10}\n",
    "    for cr in [\"gini\", \"entropy\"] :\n",
    "        for dp in range(1,15):\n",
    "            for es in range(1,201,2):\n",
    "                rdf = RandomForestClassifier(n_estimators=es, max_depth=dp, criterion=cr)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], \n",
    "                                                        test_size=0.2, random_state=1)\n",
    "\n",
    "                rdf.fit(X_train, y_train)\n",
    "                acc = get_accuracy(adb, { \"x\" : X_test, \"y\" : y_test })\n",
    "\n",
    "                if acc > best_options[\"val\"] :\n",
    "                    best_options[\"val\"] = acc\n",
    "                    best_options[\"ests\"] = es\n",
    "                    best_options[\"crit\"] = cr\n",
    "                    best_options[\"depth\"] = dp\n",
    "    return best_options\n",
    "\n",
    "def create_rdf(features, path):\n",
    "    df = pd.read_csv(str(path))\n",
    "    best_options = best_ada_crit(features, df)\n",
    "\n",
    "    rdf = RandomForestClassifier(n_estimators=es, max_depth=dp, criterion=cr)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df[features], df[target], \n",
    "                                            test_size=0.2, random_state=1)\n",
    "\n",
    "    rdf.fit(X_train, y_train)\n",
    "    acc = get_accuracy(rdf, { \"x\" : X_test, \"y\" : y_test })\n",
    "    \n",
    "    print(\"ACC: \",acc, \" Estimators: \",best_options['ests'], \" Criterion: \",best_options['crit'], \" Depth: \",                       best_options['depth'])\n",
    "    print_features(rdf)\n",
    "\n",
    "create_rdf(features, data_old_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}